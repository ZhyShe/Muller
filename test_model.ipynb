{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entry_stop_gradients(target, mask):\n",
    "    mask_stop = tf.logical_not(mask)\n",
    "    mask = tf.cast(mask, dtype=target.dtype)\n",
    "    mask_stop = tf.cast(mask_stop, dtype=target.dtype)\n",
    "    return tf.stop_gradient(mask_stop * target) + mask * target\n",
    "\n",
    "\n",
    "class affine_coupling(layers.Layer):\n",
    "    def __init__(self, name, n_split_at, n_width=32, flow_coupling=0, **kwargs):\n",
    "        super(affine_coupling, self).__init__(name=name, **kwargs)\n",
    "        self.n_split_at = n_split_at\n",
    "        self.flow_coupling = flow_coupling\n",
    "        self.n_width = n_width\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n_length = input_shape[-1]\n",
    "        if self.flow_coupling == 0:\n",
    "            self.f = NN2('a2b', self.n_width, n_length-self.n_split_at)\n",
    "        elif self.flow_coupling == 1:\n",
    "            self.f = NN2('a2b', self.n_width, (n_length-self.n_split_at)*2)\n",
    "        else:\n",
    "            raise Exception()\n",
    "        self.log_gamma = self.add_weight(name='log_gamma', shape=(\n",
    "            1, n_length-self.n_split_at), initializer=tf.zeros_initializer(), dtype=tf.float32, trainable=True)\n",
    "\n",
    "    def call(self, x, logdet=None):\n",
    "        z = x\n",
    "        n_split_at = self.n_split_at\n",
    "        alpha = 0.6\n",
    "\n",
    "        z1, z2 = z[:, :n_split_at], z[:, n_split_at:]\n",
    "\n",
    "        if self.flow_coupling == 0:\n",
    "            shift = self.f(z1)\n",
    "            shift = tf.exp(self.log_gamma)*tf.nn.tanh(shift)\n",
    "            z2 += shift\n",
    "        elif self.flow_coupling == 1:\n",
    "            h = self.f(z1)\n",
    "            shift = h[:, ::2]\n",
    "            scale = alpha*tf.nn.tanh(h[:, 1::2])\n",
    "            shift = tf.exp(self.log_gamma)*tf.nn.tanh(shift)\n",
    "            z2 = z2 + scale*z2 + shift\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid flow_coupling value. Use 0 (additive) or 1 (affine).\")\n",
    "        if logdet is not None:\n",
    "            logdet += tf.reduce_sum(tf.math.log(scale +\n",
    "                                    tf.ones_like(scale)), axis=[1], keepdims=True)\n",
    "\n",
    "        z = tf.concat([z1, z2], 1)\n",
    "        if logdet is not None:\n",
    "            return z, logdet\n",
    "\n",
    "        return z\n",
    "\n",
    "    def inverse(self, z, logdet=None):\n",
    "        n_split_at = self.n_split_at\n",
    "        z1, z2 = z[:, :n_split_at], z[:, n_split_at:]\n",
    "        alpha = 0.6\n",
    "\n",
    "        if self.flow_coupling == 0:\n",
    "            shift = self.f(z1)\n",
    "            shift = tf.exp(self.log_gamma)*tf.nn.tanh(shift)\n",
    "            z2 -= shift\n",
    "        elif self.flow_coupling == 1:\n",
    "            h = self.f(z1)\n",
    "            shift = h[:, ::2]\n",
    "            scale = alpha*tf.nn.tanh(h[:, 1::2])\n",
    "            shift = tf.exp(self.log_gamma)*tf.nn.tanh(shift)\n",
    "            z2 = (z2 - shift) / (tf.ones_like(scale) + scale)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid flow_coupling value. Use 0 (additive) or 1 (affine).\")\n",
    "        if logdet is not None:\n",
    "            logdet -= tf.reduce_sum(tf.math.log(scale +\n",
    "                                    tf.ones_like(scale)), axis=[1], keepdims=True)\n",
    "\n",
    "        z = tf.concat([z1, z2], 1)\n",
    "        if logdet is not None:\n",
    "            return z, logdet\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "class NN2(layers.Layer):\n",
    "    def __init__(self, name, n_width=32, n_out=None, **kwargs):\n",
    "        super(NN2, self).__init__(name=name, **kwargs)\n",
    "        self.n_width = n_width\n",
    "        self.n_out = n_out\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.l_1 = layers.Dense(units=self.n_width, activation=None, name='h1')\n",
    "        self.l_2 = layers.Dense(units=self.n_width, activation=None, name='h2')\n",
    "        n_out = self.n_out or int(input_shape[-1])\n",
    "        self.l_f = layers.Dense(units=n_out, activation=None, name='last')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.l_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.l_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.l_f(x)\n",
    "        # #Option for Tanh with high regularity (commented out)\n",
    "        # x = tf.nn.tanh(self.l_1(inputs))\n",
    "        # x = tf.nn.tanh(self.l_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class NN2v(layers.Layer):\n",
    "    def __init__(self, name, n_width=32, n_out=None, **kwargs):\n",
    "        super(NN2v, self).__init__(name=name, **kwargs)\n",
    "        self.n_width = n_width\n",
    "        self.n_out = n_out\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.l_1 = layers.Dense(units=self.n_width, activation=None, name='h1')\n",
    "        self.l_2 = layers.Dense(units=self.n_width//2,\n",
    "                                activation=None, name='h2')\n",
    "        self.l_3 = layers.Dense(units=self.n_width//2,\n",
    "                                activation=None, name='h3')\n",
    "        self.l_4 = layers.Dense(units=self.n_width, activation=None, name='h4')\n",
    "        n_out = self.n_out or int(input_shape[-1])\n",
    "        self.l_f = layers.Dense(units=n_out, activation=None, name='last')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.nn.relu(self.l_1(inputs))\n",
    "        x = tf.nn.relu(self.l_2(x))\n",
    "        x = tf.nn.relu(self.l_3(x))\n",
    "        x = tf.nn.relu(self.l_4(x))\n",
    "        x = self.l_f(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class squeezing(layers.Layer):\n",
    "    def __init__(self, name, n_dim, n_cut=1, **kwargs):\n",
    "        super(squeezing, self).__init__(name=name, **kwargs)\n",
    "        self.n_dim = n_dim\n",
    "        self.n_cut = n_cut\n",
    "        self.x = None\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z = inputs\n",
    "        n_length = z.get_shape()[-1]\n",
    "\n",
    "        if self.n_length < self.n_cut and not self.x:\n",
    "            raise Exception()\n",
    "        if self.n_dim == n_length:\n",
    "            if n_length > self.n_cut:\n",
    "                if self.x:\n",
    "                    raise Exception()\n",
    "                else:\n",
    "                    self.x = z[:, n_length - self.n_cut:]\n",
    "                    z = z[:, :n_length - self.n_cut]\n",
    "            else:\n",
    "                self.x = None\n",
    "        elif self.n_length <= self.n_cut:\n",
    "            z = tf.concat([z, self.x], 1)\n",
    "            self.x = None\n",
    "        else:\n",
    "            cut = z[:, n_length - self.n_cut:]\n",
    "            self.x = tf.concat([cut, self.x], 1)\n",
    "            z = z[:, :n_length - self.n_cut]\n",
    "        return z\n",
    "\n",
    "    def inverse(self, inputs):\n",
    "        z = inputs\n",
    "        n_length = z.get_shape()[-1]\n",
    "\n",
    "        if self.n_dim == n_length:\n",
    "            n_start = self.n_dim % self.n_cut\n",
    "            n_start += self.n_cut if n_start == 0 else 0\n",
    "            self.x = z[:, n_start:]\n",
    "            z = z[:, :n_start]\n",
    "        else:\n",
    "            x_length = self.x.get_shape()[-1]\n",
    "            if x_length < self.n_cut:\n",
    "                raise Exception()\n",
    "            cut = self.x[:, :self.n_cut]\n",
    "            z = tf.concat([z, cut], 1)\n",
    "            if x_length - self.n_cut == 0:\n",
    "                self.x = None\n",
    "            else:\n",
    "                self.x = self.x[:, self.n_cut:]\n",
    "        return z\n",
    "\n",
    "\n",
    "class squeezing2(layers.Layer):\n",
    "    def __init__(self, name, n_dim, n_cut=1, **kwargs):\n",
    "        super(squeezing2, self).__init__(name=name, **kwargs)\n",
    "        self.n_dim = n_dim\n",
    "        self.n_cut = n_cut\n",
    "        self.x = None\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z = inputs\n",
    "        # print(z.get_shape())\n",
    "        n_length = z.get_shape()[-1]\n",
    "\n",
    "        if n_length < self.n_cut and not self.x:\n",
    "            raise Exception()\n",
    "\n",
    "        if self.n_dim == n_length:\n",
    "            if n_length > 2*self.n_cut:\n",
    "                if self.x != None:\n",
    "                    raise Exception()\n",
    "                else:\n",
    "                    self.x = z[:, n_length - self.n_cut:]\n",
    "                    z = z[:, :n_length - self.n_cut]\n",
    "            else:\n",
    "                self.x = None\n",
    "        elif n_length <= 2*self.n_cut:\n",
    "            z = tf.concat([z, self.x], 1)\n",
    "            self.x = None\n",
    "        else:\n",
    "            cut = z[:, n_length - self.n_cut:]\n",
    "            self.x = tf.concat([cut, self.x], 1)\n",
    "            z = z[:, :n_length - self.n_cut]\n",
    "        return z\n",
    "\n",
    "    def inverse(self, inputs):\n",
    "        z = inputs\n",
    "        n_length = z.get_shape()[-1]\n",
    "        if self.n_dim == n_length:\n",
    "            n_start = self.n_dim % self.n_cut\n",
    "            if n_start == 0:\n",
    "                n_start += self.n_cut\n",
    "            self.x = z[:, n_start:]\n",
    "            z = z[:, :n_start]\n",
    "\n",
    "        x_length = self.x.get_shape()[-1]\n",
    "        if x_length < self.n_cut:\n",
    "            raise Exception()\n",
    "        cut = self.x[:, :self.n_cut]\n",
    "        z = tf.concat([z, cut], 1)\n",
    "        if x_length - self.n_cut == 0:\n",
    "            self.x = None\n",
    "        else:\n",
    "            self.x = self.x[:, self.n_cut:]\n",
    "        return z\n",
    "\n",
    "\n",
    "class W_LU(layers.Layer):\n",
    "    def __init__(self, name, **kwargs):\n",
    "        super(W_LU, self).__init__(name=name, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.n_length = input_shape[-1]\n",
    "        self.LU = self.add_weight(name='LU', shape=(self.n_length, self.n_length),initializer=tf.zeros_initializer(), dtype=tf.float32, trainable=True)\n",
    "        self.LU_init = self.add_weight(name=\"LU_init\", shape=(\n",
    "            self.n_length, self.n_length), initializer=tf.keras.initializers.Identity(), trainable=False, dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs, logdet=None, reverse=False):\n",
    "        x = inputs\n",
    "        n_dim = x.shape[-1]\n",
    "        LU = self.LU_init + self.LU\n",
    "\n",
    "        U = tf.linalg.band_part(LU, 0, -1)\n",
    "        U_diag = tf.linalg.tensor_diag_part(U)\n",
    "        U_mask = (tf.linalg.band_part(tf.ones([n_dim, n_dim]), 0, -1) >= 1)\n",
    "        U = entry_stop_gradients(U, U_mask)\n",
    "\n",
    "        I = tf.eye(self.n_length, dtype=tf.float32)\n",
    "        L = tf.linalg.band_part(I+LU, -1, 0)-tf.linalg.band_part(LU, 0, 0)\n",
    "        L_mask = (tf.linalg.band_part(tf.ones(\n",
    "            [n_dim, n_dim]), -1, 0) - tf.linalg.band_part(tf.ones([n_dim, n_dim]), 0, 0) >= 1)\n",
    "        L = entry_stop_gradients(L, L_mask)\n",
    "\n",
    "        if not reverse:\n",
    "            x = tf.transpose(x)\n",
    "            x = tf.linalg.matmul(U, x)\n",
    "            x = tf.linalg.matmul(L, x)\n",
    "            x = tf.transpose(x)\n",
    "        else:\n",
    "            x = tf.transpose(x)\n",
    "            x = tf.linalg.matmul(tf.linalg.inv(L), x)\n",
    "            x = tf.linalg.matmul(tf.linalg.inv(U), x)\n",
    "            x = tf.transpose(x)\n",
    "\n",
    "        if logdet is not None:\n",
    "            dlogdet = tf.reduce_sum(tf.math.log(tf.math.abs(U_diag)))\n",
    "            if reverse:\n",
    "                dlogdet *= -1.0\n",
    "            return x, logdet + dlogdet\n",
    "        return x\n",
    "\n",
    "\n",
    "class flow_mapping(layers.Layer):\n",
    "    def __init__(self, name, n_depth, n_split_at, n_width=32, flow_coupling=0, n_bins=16, **kwargs):\n",
    "        super(flow_mapping, self).__init__(name=name, **kwargs)\n",
    "        self.n_depth = n_depth\n",
    "        self.n_split_at = n_split_at\n",
    "        self.n_width = n_width\n",
    "        self.flow_coupling = flow_coupling\n",
    "        self.n_bins = n_bins\n",
    "        assert n_depth % 2 == 0\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.n_length = input_shape[-1]\n",
    "        self.affine_layers = []\n",
    "        self.scale_layers = []\n",
    "\n",
    "        sign = -1\n",
    "        for i in range(self.n_depth):\n",
    "            self.scale_layers.append(actnorm('actnorm'+str(i)))\n",
    "            sign *= -1\n",
    "            i_split_at = (self.n_split_at*sign + self.n_length) % self.n_length\n",
    "            self.affine_layers.append(affine_coupling('af_coupling_' + str(i),i_split_at,n_width=self.n_width,flow_coupling=self.flow_coupling))\n",
    "            # if self.n_bins > 0:\n",
    "            #   self.cdf_layer = CDF_quadratic('cdf_layer', self.n_bins)\n",
    "\n",
    "    def call(self, inputs, logdet=None, reverse=False):\n",
    "        z = inputs\n",
    "        if not reverse:\n",
    "            for i in range(self.n_depth):\n",
    "                z = self.scale_layers[i](z,logdet)\n",
    "                if logdet is not None:\n",
    "                    z, logdet = z\n",
    "                \n",
    "                z = self.affine_layers[i](z, logdet)\n",
    "                if logdet is not None:\n",
    "                    z, logdet = z\n",
    "                z = z[:, ::-1]\n",
    "        else:\n",
    "            for i in reversed(range(self.n_depth)):\n",
    "                z = z[:, ::-1]\n",
    "                z = self.affine_layers[i].inverse(z, logdet)\n",
    "                if logdet is not None:\n",
    "                    z, logdet = z\n",
    "                \n",
    "                z = self.scale_layers[i](z, logdet, reverse=True)\n",
    "                if logdet is not None:\n",
    "                    z, logdet = z\n",
    "        if logdet is not None:\n",
    "            return z, logdet\n",
    "        return z\n",
    "    def actnorm_data_initialization(self):\n",
    "        for i in range(self.n_depth):\n",
    "            self.scale_layers[i].reset_data_initialization()\n",
    "\n",
    "class actnorm(layers.Layer):\n",
    "    def __init__(self,name,scale = 1.0,logscale_factor = 3.0,**kwargs):\n",
    "        super(actnorm, self).__init__(name=name,**kwargs)\n",
    "        self.scale = scale\n",
    "        self.logscale_factor = logscale_factor\n",
    "        self.data_init = True\n",
    "    def build(self, input_shape):\n",
    "        self.n_length = input_shape[-1]\n",
    "        self.b = self.add_weight(name='b', shape=(1, self.n_length),initializer=tf.zeros_initializer(),dtype=tf.float32, trainable=True)\n",
    "        self.b_init = self.add_weight(name='b_init', shape=(1, self.n_length),initializer=tf.zeros_initializer(),dtype=tf.float32, trainable=False)\n",
    "        self.logs  = self.add_weight(name='logs', shape=(1, self.n_length),initializer=tf.zeros_initializer(),dtype=tf.float32, trainable=True)\n",
    "        self.logs_init = self.add_weight(name='logs_init', shape=(1, self.n_length),initializer=tf.zeros_initializer(),dtype=tf.float32, trainable=False)\n",
    "        \n",
    "    def call(self, inputs, logdet = None, reverse = False):\n",
    "        # data initialization\n",
    "        # # by default, no data initialization is implemented.\n",
    "        if not self.data_init:\n",
    "            x_mean = tf.reduce_mean(inputs, [0], keepdims=True)\n",
    "            x_var = tf.reduce_mean(tf.square(inputs-x_mean), [0], keepdims=True)\n",
    "            \n",
    "            self.b_init.assign(-x_mean)\n",
    "            self.logs_init.assign(tf.math.log(self.scale/(tf.sqrt(x_var)+1e-6))/self.logscale_factor)\n",
    "            \n",
    "            self.data_init = True\n",
    "        if not reverse:\n",
    "            x = inputs + (self.b + self.b_init)\n",
    "            x = x * tf.exp(self.logs + self.logs_init)\n",
    "        else:\n",
    "            x = inputs * tf.exp(-self.logs - self.logs_init)\n",
    "            x = x - (self.b + self.b_init)\n",
    "        \n",
    "        if logdet is not None:\n",
    "            dlogdet = tf.reduce_sum(self.logs + self.logs_init)\n",
    "            if reverse:\n",
    "                dlogdet *= -1\n",
    "            return x, logdet + dlogdet\n",
    "        \n",
    "        return x\n",
    "    def reset_data_initialization(self):\n",
    "        self.data_init = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class krnet(tf.keras.Model):\n",
    "    def __init__(self, name, n_dim, n_step, n_depth, n_width=32, shrink_rate=1.0, flow_coupling=0, n_bins=16, rotation=False, **kwargs):\n",
    "        super(krnet, self).__init__(name=name, **kwargs)\n",
    "        assert n_depth % 2 == 0\n",
    "        # tfd = tfp.distributions\n",
    "        # prior = tfd.MultivariateNormalDiag(loc=tf.zeros(n_dim),scale_diag=tf.ones(n_dim))\n",
    "\n",
    "        self.n_dim = n_dim\n",
    "        self.Max = tf.convert_to_tensor(np.array([[-3,0],[0.00,0],[3,0]]),dtype=tf.float32)\n",
    "        self.n_step = n_step\n",
    "        self.n_depth = n_depth\n",
    "        self.n_width = n_width\n",
    "        self.n_bins = n_bins\n",
    "        self.shrink_rate = shrink_rate\n",
    "        self.flow_coupling = flow_coupling\n",
    "        self.rotation = rotation\n",
    "        ######\n",
    "        self.weights_1 = [0.5, 0.2,0.3]\n",
    "        self.means_1 = [-3, 0, 3]\n",
    "        self.stds_1 = [1.0, 0.5, 0.8]\n",
    "\n",
    "        ######\n",
    "\n",
    "        self.n_stage = n_dim // n_step\n",
    "        if n_dim % n_step == 0:\n",
    "            self.n_stage -= 1\n",
    "        self.n_rotation = self.n_stage\n",
    "\n",
    "        if rotation:\n",
    "            self.rotations = []\n",
    "            for i in range(self.n_rotation):\n",
    "                self.rotations.append(W_LU('rotation'+str(i)))\n",
    "\n",
    "        self.flow_mappings = []\n",
    "        for i in range(self.n_stage):\n",
    "            n_split_at = n_dim - (i+1) * n_step\n",
    "            self.flow_mappings.append(flow_mapping('flow_mapping'+str(i),\n",
    "                                                   n_depth,\n",
    "                                                   n_split_at,\n",
    "                                                   n_width=n_width,\n",
    "                                                   flow_coupling=flow_coupling,\n",
    "                                                   n_bins=n_bins))\n",
    "            n_width = int(n_width*self.shrink_rate)\n",
    "\n",
    "        self.squeezing_layer = squeezing2('squeezing', n_dim, n_step)\n",
    "        # self.log_prior = self.prior.log_prob\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # print('call invoke')\n",
    "        objective = tf.zeros_like(inputs, dtype='float32')[:, 0]\n",
    "        objective = tf.reshape(objective, [-1, 1])\n",
    "        z, objective = self.mapping_to_prior(inputs, objective)\n",
    "        objective += tf.reshape(self.compute_prior_log_prob(z), [-1, 1])\n",
    "        return objective\n",
    "\n",
    "    def compute_prior_log_prob(self, samples):\n",
    "        \"\"\"\n",
    "        Compute the log probability under the mixture Gaussian prior.\n",
    "        :param samples: Tensor of shape (n_samples, n_dim).\n",
    "        :return: Tensor of log probabilities of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        dim_1 = samples[:, 0]\n",
    "        # dim_2 = samples[:, 1]\n",
    "        # dim_3 = samples[:, 2]\n",
    "        gaussian_dims = samples[:, 1:]\n",
    "\n",
    "        # Compute mixture Gaussian log probabilities for first three dimensions\n",
    "        log_pdf_1 = self._compute_mixture_pdf(\n",
    "            dim_1, self.weights_1, self.means_1, self.stds_1)\n",
    "        # log_pdf_2 = self._compute_mixture_pdf(\n",
    "        #     dim_2, self.weights_2, self.means_2, self.stds_2)\n",
    "        # log_pdf_3 = self._compute_mixture_pdf(\n",
    "        #     dim_3, self.weights_3, self.means_3, self.stds_3)\n",
    "\n",
    "        # Compute Gaussian log PDF for remaining dimensions\n",
    "        log_pdf_gaussian = tf.reduce_sum(-0.5 * gaussian_dims **\n",
    "                                         2 - tf.math.log(tf.sqrt(2 * np.pi)), axis=1)\n",
    "\n",
    "        # Combine log probabilities\n",
    "        log_prior = log_pdf_1 + log_pdf_gaussian #+ log_pdf_2 + log_pdf_3\n",
    "        return log_prior\n",
    "\n",
    "    def _compute_mixture_pdf(self, samples, weights, means, stds):\n",
    "        \"\"\"\n",
    "        Compute the log PDF for a mixture of Gaussians in a numerically stable way.\n",
    "        :param samples: Input tensor of shape (n_samples,).\n",
    "        :param weights: List or Tensor of mixture weights.\n",
    "        :param means: List or Tensor of means for each Gaussian component.\n",
    "        :param stds: List or Tensor of standard deviations for each Gaussian component.\n",
    "        :return: Log PDF of the mixture for each sample.\n",
    "        \"\"\"\n",
    "        weights = tf.constant(weights, dtype=tf.float32)\n",
    "        means = tf.constant(means, dtype=tf.float32)\n",
    "        stds = tf.constant(stds, dtype=tf.float32)\n",
    "\n",
    "        # Expand dimensions to match samples shape for broadcasting\n",
    "        samples = tf.expand_dims(samples, axis=-1)  # Shape: (n_samples, 1)\n",
    "        means = tf.expand_dims(means, axis=0)       # Shape: (1, n_components)\n",
    "        stds = tf.expand_dims(stds, axis=0)         # Shape: (1, n_components)\n",
    "\n",
    "        # Compute log probabilities for each component\n",
    "        log_probs = -0.5 * ((samples - means) / stds) ** 2 - \\\n",
    "            tf.math.log(stds * tf.sqrt(2 * np.pi))\n",
    "        log_probs += tf.math.log(weights)  # Add log weights\n",
    "\n",
    "        # Combine log probabilities using log-sum-exp for numerical stability\n",
    "        return tf.reduce_logsumexp(log_probs, axis=-1)  # Shape: (n_samples,)\n",
    "\n",
    "    def mapping_to_prior(self, inputs, logdet=None):\n",
    "        z = inputs\n",
    "        # print(\"Mapping to prior, input shape:\", z.shape)\n",
    "        for i in range(self.n_stage):\n",
    "            # print(f\"Stage {i}, input shape before squeezing: {z.shape}\")\n",
    "            if logdet is not None:\n",
    "                if self.rotation and i < self.n_rotation:\n",
    "                    z, logdet = self.rotations[i](z, logdet)\n",
    "                    # print(f\"Applied rotation {i}, output shape: {z.shape}\")\n",
    "                z, logdet = self.flow_mappings[i](z, logdet)\n",
    "                # print(f\"Applied flow mapping {i}, output shape: {z.shape}\")\n",
    "            else:\n",
    "                if self.rotation and i < self.n_rotation:\n",
    "                    z = self.rotations[i](z)\n",
    "                z = self.flow_mappings[i](z)\n",
    "            z = self.squeezing_layer(z)\n",
    "        # print(\"Final shape after mapping to prior:\", z.shape)\n",
    "        if logdet is not None:\n",
    "            return z, logdet\n",
    "        return z\n",
    "    \n",
    "    def actnorm_data_initialization(self):\n",
    "        for i in range(self.n_stage):\n",
    "            self.flow_mappings[i].actnorm_data_initialization()\n",
    "\n",
    "    def mapping_from_prior(self, inputs):\n",
    "        z = inputs\n",
    "        for i in reversed(range(self.n_stage)):\n",
    "            z = self.squeezing_layer.inverse(z)\n",
    "            z = self.flow_mappings[i](z, reverse=True)\n",
    "            if self.rotation and i < self.n_rotation:\n",
    "                z = self.rotations[i](z, reverse=True)\n",
    "        return z\n",
    "\n",
    "    def draw_samples_from_prior(self, n_samples):\n",
    "        \"\"\"\n",
    "        Generate samples from the prior distribution.\n",
    "        :param n_samples: Number of samples to generate.\n",
    "        :return: TensorFlow tensor of shape (n_samples, n_dim).\n",
    "        \"\"\"\n",
    "        # First 3 dimensions: Mixture Gaussians\n",
    "        dim_1 = self._generate_mixture_samples(\n",
    "            n_samples, self.weights_1, self.means_1, self.stds_1)\n",
    "        # dim_2 = self._generate_mixture_samples(\n",
    "        #     n_samples, self.weights_2, self.means_2, self.stds_2)\n",
    "        # dim_3 = self._generate_mixture_samples(\n",
    "        #     n_samples, self.weights_3, self.means_3, self.stds_3)\n",
    "\n",
    "        # Last (n_dim - 3) dimensions: Standard Gaussian\n",
    "        gaussian_dims = tf.random.normal(\n",
    "            shape=(n_samples, self.n_dim - 1), mean=0.0, stddev=1.0)\n",
    "\n",
    "        # Combine all dimensions into a single tensor of shape (n_samples, n_dim)\n",
    "        random_variable = tf.concat([\n",
    "            tf.expand_dims(dim_1, axis=-1),  # Shape: (n_samples, 1)\n",
    "            gaussian_dims                     # Shape: (n_samples, n_dim - 3)\n",
    "        ], axis=1)\n",
    "\n",
    "        return self.mapping_from_prior(random_variable)\n",
    "\n",
    "    def _generate_mixture_samples(self, n_samples, weights, means, stds):\n",
    "        \"\"\"\n",
    "        Generate samples from a 1D mixture of Gaussians.\n",
    "        :param n_samples: Number of samples to generate.\n",
    "        :param weights: List of mixture weights.\n",
    "        :param means: List of means for each Gaussian component.\n",
    "        :param stds: List of standard deviations for each Gaussian component.\n",
    "        :return: TensorFlow tensor of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        # Convert weights, means, and stds to TensorFlow tensors\n",
    "        weights = tf.constant(weights, dtype=tf.float32)\n",
    "        means = tf.constant(means, dtype=tf.float32)\n",
    "        stds = tf.constant(stds, dtype=tf.float32)\n",
    "\n",
    "        # Number of components in the mixture\n",
    "        n_components = tf.shape(weights)[0]\n",
    "\n",
    "        # Sample component indices based on the mixture weights\n",
    "        components = tf.random.categorical(tf.math.log(\n",
    "            [weights]), n_samples)  # Shape: (1, n_samples)\n",
    "        components = tf.reshape(components, [-1])  # Shape: (n_samples,)\n",
    "\n",
    "        # Gather means and standard deviations for the sampled components\n",
    "        sampled_means = tf.gather(means, components)  # Shape: (n_samples,)\n",
    "        sampled_stds = tf.gather(stds, components)    # Shape: (n_samples,)\n",
    "\n",
    "        # Sample from the corresponding Gaussian components\n",
    "        samples = tf.random.normal(shape=(\n",
    "            n_samples,), mean=sampled_means, stddev=sampled_stds)  # Shape: (n_samples,)\n",
    "\n",
    "        return samples\n",
    "\n",
    "    # def draw_samples_from_prior(self, n_samples):\n",
    "    #   z = self.prior.sample((n_samples,))\n",
    "    #   x = self.mapping_from_prior(z)\n",
    "    #   return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_samples_from_prior(n_samples):\n",
    "        \"\"\"\n",
    "        Generate samples from the prior distribution.\n",
    "        :param n_samples: Number of samples to generate.\n",
    "        :return: TensorFlow tensor of shape (n_samples, n_dim).\n",
    "        \"\"\"\n",
    "        # First 3 dimensions: Mixture Gaussians\n",
    "        dim_1 = flow._generate_mixture_samples(\n",
    "            n_samples, flow.weights_1, flow.means_1, flow.stds_1)\n",
    "    \n",
    "        # Last (n_dim - 3) dimensions: Standard Gaussian\n",
    "        gaussian_dims = tf.random.normal(\n",
    "            shape=(n_samples, flow.n_dim - 1), mean=0.0, stddev=1)\n",
    "\n",
    "        # Combine all dimensions into a single tensor of shape (n_samples, n_dim)\n",
    "        random_variable = tf.concat([\n",
    "            tf.expand_dims(dim_1, axis=-1),  # Shape: (n_samples, 1)\n",
    "            gaussian_dims\n",
    "        ], axis=1)\n",
    "\n",
    "        return random_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = krnet('krnet',2,1,32,n_width=128,n_bins= 0,shrink_rate=1,flow_coupling=1,rotation=True)\n",
    "#2,1,64,256\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "dummy_input = tf.random.normal((1, 2))  # Adjust shape if necessary\n",
    "flow(dummy_input) \n",
    "\n",
    "flow.actnorm_data_initialization()\n",
    "_ = flow(dummy_input)\n",
    "gen_samp = draw_samples_from_prior(10000)\n",
    "\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,net = flow )\n",
    "checkpoint_dir = './checkpoints_3dim_new_new'\n",
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "if latest_checkpoint:\n",
    "    checkpoint.restore(latest_checkpoint).expect_partial()  # Restore the checkpoint into flow1\n",
    "    print(optimizer.learning_rate.numpy())\n",
    "    print(f\"Restored checkpoint from {latest_checkpoint}\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting from scratch.\")\n",
    "    flow.actnorm_data_initialization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_samples = flow.draw_samples_from_prior(4000000).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_samples[:,0],x_samples[:,1])\n",
    "plt.scatter(data_t[:,0],data_t[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_Max = flow.mapping_from_prior(flow.Max)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(g_Max)\n",
    "    log_pdf = flow(g_Max)\n",
    "gradients = tape.gradient(log_pdf, g_Max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muller-Brown potential parameters\n",
    "A = [-200, -100, -170, 15]\n",
    "a = [-1, -1, -6.5, 0.7]\n",
    "b = [0, 0, 11, 0.6]\n",
    "c = [-10, -10, -6.5, 0.7]\n",
    "x0 = [1, 0, -0.5, -1]\n",
    "y0 = [0, 0.5, 1.5, 1]\n",
    "\n",
    "def muller_brown_potential(x, y):\n",
    "    \"\"\"Calculate Muller-Brown potential at point (x,y)\"\"\"\n",
    "    V = 0\n",
    "    for i in range(4):\n",
    "        V += A[i] * np.exp(\n",
    "            a[i]*(x - x0[i])**2 + \n",
    "            b[i]*(x - x0[i])*(y - y0[i]) + \n",
    "            c[i]*(y - y0[i])**2\n",
    "        )\n",
    "    return V"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
